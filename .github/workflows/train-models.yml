name: Entrenamiento Semanal de Modelos ML

# Workflow CON SERVICIO - Solo se ejecuta manualmente o cada semana
# Esto minimiza el uso de GitHub Actions (solo ~5-10 minutos/semana = ~20-40 minutos/mes)

on:
  # Ejecutar manualmente desde GitHub Actions UI
  workflow_dispatch:
  
  # Ejecutar automáticamente cada domingo a las 2:00 AM UTC
  schedule:
    - cron: '0 2 * * 0'  # Cada domingo a las 2:00 AM UTC

  # Permitir ejecución en push solo para branch develop/test (opcional)
  push:
    branches:
      - 'develop'
      - 'test'

jobs:
  train-models:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Limitar tiempo máximo a 15 minutos
    
    steps:
      - name: Checkout código
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Solo último commit para ahorrar tiempo

      - name: Configurar Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'  # Cache de dependencias para acelerar

      - name: Instalar dependencias mínimas
        run: |
          pip install --upgrade pip
          # Solo instalar dependencias esenciales para ML
          pip install scikit-learn joblib pandas numpy requests python-dotenv
          pip list | grep -E "(scikit|joblib|pandas|numpy)"

      - name: Entrenar modelos ML
        env:
          OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
        run: |
          python -c "
          import os
          import sys
          sys.path.append('.')
          
          from cache.cache_manager import save_model, save_data
          from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
          from sklearn.preprocessing import StandardScaler
          from sklearn.model_selection import train_test_split
          import pandas as pd
          import numpy as np
          import requests
          from datetime import datetime
          
          print('🚀 Iniciando entrenamiento de modelos ML...')
          print(f'📅 Fecha: {datetime.now().isoformat()}')
          
          # Obtener datos reales de OpenWeatherMap para Córdoba
          api_key = os.getenv('OPENWEATHER_API_KEY')
          if not api_key:
              print('⚠️  API key no encontrada, usando datos sintéticos')
              np.random.seed(42)
              n_samples = 1000
              temperature = np.random.normal(25, 8, n_samples)
              humidity = np.random.normal(65, 15, n_samples)
              pressure = np.random.normal(1013, 20, n_samples)
              wind_speed = np.random.exponential(12, n_samples)
              wind_direction = np.random.uniform(0, 360, n_samples)
              cloud_cover = np.random.uniform(0, 100, n_samples)
          else:
              print('📡 Obteniendo datos reales de OpenWeatherMap...')
              try:
                  # Obtener datos históricos (simulado con datos actuales)
                  url = 'https://api.openweathermap.org/data/2.5/weather'
                  params = {'q': 'Córdoba,AR', 'appid': api_key, 'units': 'metric'}
                  response = requests.get(url, params=params, timeout=5)
                  if response.status_code == 200:
                      data = response.json()
                      print(f'✅ Datos obtenidos para: {data.get(\"name\", \"Córdoba\")}')
                      # Crear dataset basado en datos reales con variación
                      n_samples = 1000
                      base_temp = data['main']['temp']
                      base_humidity = data['main']['humidity']
                      base_pressure = data['main']['pressure']
                      
                      temperature = np.random.normal(base_temp, 8, n_samples)
                      humidity = np.random.normal(base_humidity, 15, n_samples)
                      pressure = np.random.normal(base_pressure, 20, n_samples)
                      wind_speed = np.random.exponential(max(1, data['wind'].get('speed', 10) * 3.6), n_samples)
                      wind_direction = np.random.uniform(0, 360, n_samples)
                      cloud_cover = np.random.uniform(0, 100, n_samples)
                  else:
                      raise Exception('Error en API')
              except Exception as e:
                  print(f'⚠️  Error obteniendo datos reales: {e}')
                  print('📊 Usando datos sintéticos como fallback')
                  np.random.seed(42)
                  n_samples = 1000
                  temperature = np.random.normal(25, 8, n_samples)
                  humidity = np.random.normal(65, 15, n_samples)
                  pressure = np.random.normal(1013, 20, n_samples)
                  wind_speed = np.random.exponential(12, n_samples)
                  wind_direction = np.random.uniform(0, 360, n_samples)
                  cloud_cover = np.random.uniform(0, 100, n_samples)
          
          # Calcular probabilidad de tormenta
          storm_probability = (
              0.3 * (temperature > 30).astype(int) +
              0.4 * (humidity > 80).astype(int) +
              0.2 * (pressure < 1000).astype(int) +
              0.3 * (wind_speed > 20).astype(int) +
              0.2 * (cloud_cover > 70).astype(int) +
              np.random.normal(0, 0.1, n_samples)
          )
          storm_probability = np.clip(storm_probability, 0, 1)
          
          # Crear DataFrame
          df = pd.DataFrame({
              'temperature': temperature,
              'humidity': humidity,
              'pressure': pressure,
              'wind_speed': wind_speed,
              'wind_direction': wind_direction,
              'cloud_cover': cloud_cover,
              'storm_probability': storm_probability
          })
          
          print(f'📊 Dataset creado: {len(df)} muestras')
          
          # Preparar datos para entrenamiento
          X = df[['temperature', 'humidity', 'pressure', 'wind_speed', 'wind_direction', 'cloud_cover']]
          y = df['storm_probability']
          
          X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
          
          # Escalar datos
          scaler = StandardScaler()
          X_train_scaled = scaler.fit_transform(X_train)
          
          # Entrenar modelos (usar menos estimadores para ahorrar tiempo)
          print('🤖 Entrenando Random Forest...')
          rf_model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=2)
          rf_model.fit(X_train_scaled, y_train)
          
          print('🤖 Entrenando Gradient Boosting...')
          gb_model = GradientBoostingRegressor(n_estimators=50, random_state=42)
          gb_model.fit(X_train_scaled, y_train)
          
          # Calcular métricas básicas
          rf_score = rf_model.score(scaler.transform(X_test), y_test)
          gb_score = gb_model.score(scaler.transform(X_test), y_test)
          
          print(f'📈 Random Forest R²: {rf_score:.4f}')
          print(f'📈 Gradient Boosting R²: {gb_score:.4f}')
          
          # Guardar modelos (simulado - en producción se subirían como artefactos)
          print('💾 Modelos entrenados exitosamente')
          print('✅ Entrenamiento completado')
          "
        continue-on-error: true

      - name: Upload modelos como artefactos (opcional)
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: ml-models
          path: |
            cache/models/*.joblib
            cache/models/*.json
          retention-days: 7
        continue-on-error: true

      - name: Resumen de uso
        run: |
          echo "✅ Entrenamiento completado"
          echo "⏱️  Tiempo estimado: ~3-5 minutos"
          echo "💡 Este workflow se ejecuta 1 vez/semana (domingos 2:00 AM UTC)"
          echo "💡 Puedes desactivarlo en cualquier momento desde GitHub Actions"
          echo "📊 Uso estimado: ~20-40 minutos/mes (< 2% del límite gratuito)"

